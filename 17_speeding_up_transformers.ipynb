{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 17 â€“ Speeding Up Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code and solutions to the exercises in Chapter 17._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ageron/handson-mlp/blob/main/17_advanced_transformer_techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-mlp/blob/main/17_advanced_transformer_techniques.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFXIv9qNpKzt",
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires Python 3.10 or above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Colab or Kaggle, the Hugging Face Datasets library is not pre-installed so we must install it manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "if IS_COLAB or IS_KAGGLE:\n",
    "    %pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJtVEqxfpKzw"
   },
   "source": [
    "We also need PyTorch â‰¥ 2.4.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0Piq5se2pKzx"
   },
   "outputs": [],
   "source": [
    "from packaging.version import Version\n",
    "import torch\n",
    "\n",
    "assert Version(torch.__version__) >= Version(\"2.4.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter can be very slow without a hardware accelerator, so if we can find one, let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's issue a warning if there's no hardware accelerator available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cpu\":\n",
    "    print(\"Neural nets can be very slow without a hardware accelerator.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
    "              \"accelerator.\")\n",
    "    if IS_KAGGLE:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDaDoLQTpKzx"
   },
   "source": [
    "As we did in earlier chapters, let's define the default font sizes to make the figures prettier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8d4TH3NbpKzx"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Decoding at Inference Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key/Value Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cache=True\n",
      "CPU times: user 2.85 s, sys: 499 ms, total: 3.35 s\n",
      "Wall time: 3.82 s\n",
      "use_cache=False\n",
      "CPU times: user 3.42 s, sys: 1.69 s, total: 5.11 s\n",
      "Wall time: 7.92 s\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"facebook/opt-125m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "prompt = \"Once upon a time there lived\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "for use_cache in (True, False):\n",
    "    print(f\"{use_cache=}\")\n",
    "    %time model.generate(**inputs, max_new_tokens=500, do_sample=False, use_cache=use_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speculative decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "W0913 09:16:04.152000 18058 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there lived an old farmer. All he did was hunt and fish, and the time ended when he was dead. Now that he is dead, he has been replaced by a young woman. The farmer lives at the end of the road - in the middle of the road, where the trees have all snapped, so that the man behind him can never drive past. At night the trees crumble all around him, so that the old man can never leave behind his old farm. His old family are afraid that his\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\",\n",
    "                                                    device_map=\"auto\")\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\",\n",
    "                                                   device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "prompt = \"Once upon a time there lived\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(target_model.device)\n",
    "outputs = target_model.generate(**inputs, max_new_tokens=100, do_sample=True,\n",
    "                                temperature=1, assistant_model=draft_model)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LongFormer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigBird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BigBirdForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "BigBirdForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "BigBirdForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Device set to use mps:0\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 14 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2818780839443207,\n",
       "  'token': 2457,\n",
       "  'token_str': 'pain',\n",
       "  'sequence': 'She was feeling unwell so she took some pain medicine.'},\n",
       " {'score': 0.2190818041563034,\n",
       "  'token': 4793,\n",
       "  'token_str': 'cold',\n",
       "  'sequence': 'She was feeling unwell so she took some cold medicine.'},\n",
       " {'score': 0.11271752417087555,\n",
       "  'token': 36298,\n",
       "  'token_str': 'allergy',\n",
       "  'sequence': 'She was feeling unwell so she took some allergy medicine.'},\n",
       " {'score': 0.06932275742292404,\n",
       "  'token': 22195,\n",
       "  'token_str': 'cough',\n",
       "  'sequence': 'She was feeling unwell so she took some cough medicine.'},\n",
       " {'score': 0.04517913982272148,\n",
       "  'token': 35968,\n",
       "  'token_str': 'herbal',\n",
       "  'sequence': 'She was feeling unwell so she took some herbal medicine.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"google/bigbird-roberta-base\"\n",
    "pipeline = pipeline(task=\"fill-mask\", model=model_id)\n",
    "pipeline(\"She was feeling unwell so she took some [MASK] medicine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can safely ignore the warnings above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routing Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def angular_lsh(vectors, k):\n",
    "    R = torch.randn(vectors.size(-1), k // 2, device=vectors.device)\n",
    "    normalized_vectors = F.normalize(vectors, p=2.0, dim=1)\n",
    "    V_proj = normalized_vectors @ R\n",
    "    V_concat = torch.cat([V_proj, -V_proj], dim=1)\n",
    "    return torch.argmax(V_concat, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 3, 0, 2, 2, 2, 2, 1, 1, 3, 3, 1, 2, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "vectors = torch.rand(16, 512)\n",
    "angular_lsh(vectors, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected value of $\\exp(\\mathbf{w}\\cdot\\mathbf{x})$ for a given vector **x** and a random vector **w** sampled from a Gaussian distribution is $\\exp\\left(\\frac{1}{2}\\|x\\|^2\\right)$. We can test this using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5851, 1.6703, 1.6728, 1.9729, 1.7934])\n",
      "tensor([1.6516, 1.7578, 1.6594, 1.8050, 1.7751])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "d, m = 64, 1024\n",
    "W = torch.randn(d, m)\n",
    "X = torch.randn(5, d) / d ** 0.5\n",
    "R = torch.exp(X @ W)\n",
    "print(R.mean(axis=-1))\n",
    "print(torch.exp(0.5 * (X.norm(dim=-1)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty close!\n",
    "\n",
    "Now let's implement the function $\\phi(\\mathbf{x}) = \\dfrac{\\exp(\\mathbf{x} \\mathbf{W})-\\frac{1}{2}\\|\\mathbf{x}\\|^2)}{\\sqrt m}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(X, W, dim_subtract_max=(-2, -1)):\n",
    "    squared_norms = X.square().sum(dim=-1, keepdim=True)\n",
    "    X_proj = X @ W\n",
    "    max_vals = X_proj.amax(dim=dim_subtract_max, keepdim=True)\n",
    "    return torch.exp(X_proj - max_vals - squared_norms / 2) / W.size(-1) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to prove that the expected value of _Ï•_(**Q**)_Ï•_(**K**)<sup>âŠº</sup> is equal to exp(**QK**<sup>âŠº</sup>). Let's check that this is indeed the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0171)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 32\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "Lq = 200\n",
    "Lk = 100\n",
    "m = 256\n",
    "d_head = d_model // n_heads\n",
    "W = torch.randn(n_heads, d_head, m)\n",
    "Q = torch.randn(batch_size, n_heads, Lq, d_head)\n",
    "K = torch.randn(batch_size, n_heads, Lk, d_head)\n",
    "scale = 1 / d_head ** 0.5\n",
    "Qp = phi(Q * scale ** 0.5, W, dim_subtract_max=-1)\n",
    "Kp = phi(K * scale ** 0.5, W)\n",
    "A = Qp @ Kp.transpose(-2, -1)\n",
    "D = A.sum(dim=-1, keepdim=True)\n",
    "result = A / (D + 1e-6)\n",
    "expected = torch.softmax(Q @ K.transpose(-2, -1) * scale, dim=-1)\n",
    "rmse = F.mse_loss(result, expected) ** 0.5\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty good approximation! We can still improve it by orthogonalizing **W** using QR decomposition. Since there can only _d_ orthogonal vectors in a _d_-dimensional space, we orthogonalize each chunk of _d_ random vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonalize(W):\n",
    "    d_head = W.size(-2)\n",
    "    W_orth = torch.cat([torch.linalg.qr(W_chunk)[0]\n",
    "                        for W_chunk in W.split(d_head, dim=-1)], dim=-1)\n",
    "    return W_orth * d_head ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_orth = orthogonalize(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the RMSE once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0160)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qp2 = phi(Q * scale ** 0.5, W_orth, dim_subtract_max=-1)\n",
    "Kp2 = phi(K * scale ** 0.5, W_orth)\n",
    "A2 = Qp2 @ Kp2.transpose(-2, -1)\n",
    "D2 = A2.sum(dim=-1, keepdim=True)\n",
    "result2 = A2 / (D2 + 1e-6)\n",
    "rmse2 = F.mse_loss(result2, expected) ** 0.5\n",
    "rmse2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a bit better. If you increase the number of features, it will reduce this error further, at the cost of more compute and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to implement FAVOR+ attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FavorAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, n_features):\n",
    "        super().__init__()\n",
    "        self.d_head = d_model // n_heads\n",
    "        W = torch.randn(n_heads, self.d_head, n_features)  # H, D, m\n",
    "        W = orthogonalize(W)\n",
    "        self.register_buffer(\"W\", W)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        scale = self.d_head ** -0.25\n",
    "        Qp = phi(Q * scale, self.W, dim_subtract_max=-1)\n",
    "        Kp = phi(K * scale, self.W)\n",
    "        D = Qp @ Kp.sum(dim=-2).unsqueeze(-1)  # B, H, Lq, 1\n",
    "        Kp_T_V = Kp.transpose(-2, -1) @ V  # B, H, m, D\n",
    "        return (Qp @ Kp_T_V) / (D + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "Q = torch.randn(batch_size, n_heads, Lq, d_head)\n",
    "K = torch.randn(batch_size, n_heads, Lk, d_head)\n",
    "V = torch.randn(batch_size, n_heads, Lk, d_head)\n",
    "favor_attn = FavorAttention(d_model, n_heads, 256)\n",
    "approx_attn = favor_attn(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1599)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attn = F.scaled_dot_product_attention(Q, K, V)\n",
    "attn_rmse = F.mse_loss(approx_attn, attn) ** 0.5\n",
    "attn_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Projections in Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, Lq, Lk, d_head = 32, 100, 90, 64\n",
    "n_heads = 8\n",
    "n_groups = 1\n",
    "query = torch.randn(batch_size, n_heads, Lq, d_head)\n",
    "key = torch.randn(batch_size, n_groups, Lk, d_head)\n",
    "value = torch.randn(batch_size, n_groups, Lk, d_head)\n",
    "attn = F.scaled_dot_product_attention(query, key, value, enable_gqa=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, Lq, Lk, d_head = 32, 100, 90, 64\n",
    "n_heads = 8\n",
    "n_groups = 2\n",
    "query = torch.randn(batch_size, n_heads, Lq, d_head)\n",
    "key = torch.randn(batch_size, n_groups, Lk, d_head)\n",
    "value = torch.randn(batch_size, n_groups, Lk, d_head)\n",
    "attn = F.scaled_dot_product_attention(query, key, value, enable_gqa=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FlashAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a toy Python implementation of FlashAttention, to get an idea of how it works under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention(Q, K, V, block_size_q, block_size_k):\n",
    "    Lq, d = Q.shape\n",
    "    Lk, _ = K.shape\n",
    "    O = torch.zeros_like(Q)\n",
    "    scale = d ** -0.5\n",
    "\n",
    "    for i_start in range(0, Lq, block_size_q):  # iterate over query blocks\n",
    "        i_end = min(i_start + block_size_q, Lq)\n",
    "        Q_block = Q[i_start:i_end]\n",
    "        \n",
    "        O_block = torch.zeros(block_size_q, d, device=Q.device)\n",
    "        l_block = torch.zeros(block_size_q, 1, device=Q.device)\n",
    "        m_block = -torch.inf * torch.ones(block_size_q, 1, device=Q.device)\n",
    "\n",
    "        for j_start in range(0, Lk, block_size_k):  # iterate over K/V blocks\n",
    "            j_end = min(j_start + block_size_k, Lk)\n",
    "            K_block = K[j_start:j_end]\n",
    "            V_block = V[j_start:j_end]\n",
    "\n",
    "            # Core attention calculation for the block\n",
    "            S_ij = Q_block @ K_block.T * scale  # (block_size_q, block_size_k)\n",
    "\n",
    "            # Find the new maximum score for the combined blocks so far\n",
    "            m_ij_new, _ = torch.max(S_ij, dim=1, keepdim=True)\n",
    "            m_block_new = torch.maximum(m_block, m_ij_new)\n",
    "            \n",
    "            # Rescale previous accumulator values based on the new max\n",
    "            P_ij = torch.exp(S_ij - m_block_new)\n",
    "            correction_factor = torch.exp(m_block - m_block_new)\n",
    "\n",
    "            # Update the denominator (l) and output (O) accumulators\n",
    "            l_block_new = ((l_block * correction_factor)\n",
    "                           + torch.sum(P_ij, dim=1, keepdim=True))\n",
    "            O_block = (O_block * correction_factor) + (P_ij @ V_block)\n",
    "\n",
    "            # Update the state for the next inner loop iteration\n",
    "            l_block = l_block_new\n",
    "            m_block = m_block_new\n",
    "\n",
    "        # Rescale the output block and write it to the final output matrix\n",
    "        O[i_start:i_end] = O_block / l_block\n",
    "\n",
    "    return O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that this works (note that this simple implementation only handles the case where _L_<sub>Q</sub> and _L_<sub>K</sub> are multiples of the block size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "block_size_q, block_size_k = 64, 64\n",
    "Lq, Lk, d = 1280, 1152, 512\n",
    "Q = torch.randn(Lq, d)\n",
    "K = torch.randn(Lk, d)\n",
    "V = torch.randn(Lk, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "R1 = flash_attention(Q, K, V, block_size_q, block_size_k)\n",
    "R2 = F.scaled_dot_product_attention(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9284e-15)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = F.mse_loss(R1, R2)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Up With Mixture of experts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model_id = \"EleutherAI/gpt-neo-125M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\",\n",
    "                                             dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (TrainingArguments, Trainer,\n",
    "                          DataCollatorForLanguageModeling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter-efficient fine-tuning (PEFT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence packing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(10, 1).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "data_loader = [(torch.randn(8, 10), torch.randn(8, 1)) for _ in range(100)]\n",
    "model.train()\n",
    "\n",
    "accumulation_steps = 4\n",
    "optimizer.zero_grad()  # reset gradients before starting\n",
    "for batch_index, (X_batch, y_batch) in enumerate(data_loader):\n",
    "    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "    y_pred = model(X_batch)\n",
    "    loss = criterion(y_pred, y_batch)\n",
    "    loss = loss / accumulation_steps\n",
    "    loss.backward()\n",
    "    if (batch_index + 1) % accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Work in progress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
